# This configuration is inspired by Encodec paper

# input configs
input_size: 1
use_preprocessor: true      # use preprocessor to clip speech to target length
speech_max_length: 40800    # in seconds
valid_max_length: 40800        # in seconds
sampling_rate: 16000
# clip_audio_from_left_side: true

# encoder
encoder: encodec_seanet_encoder_2d
encoder_conf:
    ratios: [[4,2],[4,1],[4,2],[4,1]]
    norm: time_group_norm
    causal: false

# quantizer
quantizer: costume_quantizer_raw
quantizer_conf:
    # codebook_size: 1024
    codebook_size: 8192
    # num_quantizers: 32
    num_quantizers: 2
    ema_decay: 0.99
    kmeans_init: true
    sampling_rate: 16000
    encoder_hop_length: 640
    quantize_dropout: true
    rand_num_quant: [1, 2, 2, 2]
    use_ddp: true

# decoder
decoder: encodec_seanet_decoder
decoder_conf:
    ratios: [8, 5, 4, 2, 2]
    causal: false
    residual_kernel_size: 3
    dilation_base: 3
    n_residual_layers: 3
    norm: time_group_norm
    # norm: layer_norm
    # norm: conditional_norm
    # norm_params:
    #     condition_dim: 80
    #     base_norm_type: time_group_norm
        # base_norm_type: layer_norm


# discriminator
discriminator: multiple_disc
discriminator_conf:
    input_size: 1
    disc_conf_list:
        - name: encodec_multi_scale_stft_discriminator
          filters: 32

# model
model: freq_codec
model_conf:
    odim: 128
    multi_spectral_window_powers_of_two: [5, 6, 7, 8, 9, 10]
    target_sample_hz: 16000
    audio_normalize: true
    segment_dur: null
    overlap_ratio: null
    use_power_spec_loss: true
    codec_domain: ['mag', 'time']
    feat_match_loss_weight: 5.555
    phase_invariant_training: false
    pit_feat_loss_weight: 1
    pit_disc_loss_weight: 10
    feat_match_layer_start: 2

    timbre_strategy:
        timbre_encoder_config:
            encoder_type: speechbrain.Xvector
            input_type: wav
            model_dir: /home/admin_data/user/checkpoint/speechbrain/spkrec-xvect-voxceleb
            sampling_rate: 16000
            hidden_size: 512 # timbre encoder
            embed_dim: 512
            repeat_embed: true
            # merge_with_decoder: conditional_layer_norm
            transformed_speech_for_timbre_encoder: false
            freeze: false


# optimizer for generator
optim: adam            # optimizer type
optim_conf:             # keyword arguments for selected optimizer
    lr: 0.0003          # learning rate, the dot is necessary to make yaml load it as float not string
    betas: [0.5, 0.9]
scheduler: null       # scheduler type
scheduler_conf:         # keyword arguments for selected scheduler
    step_size: 8        # every 80k steps . Here, step_size actually means epoch rather than batch iteration
    gamma: 0.1

# optimizer for discriminator
optim2: adam             # optimizer type
optim2_conf:              # keyword arguments for selected optimizer
    lr: 0.0003              # learning rate
    betas: [0.5, 0.9]
scheduler2: null       # scheduler type
scheduler2_conf:         # keyword arguments for selected scheduler
    step_size: 8        # every 80k steps. Here, step_size actually means epoch rather than batch iteration
    gamma: 0.1

# training settings
num_iters_per_epoch: 10000 # number of iterations per epoch
max_epoch: 20            # number of epochs
accum_grad: 1             # gradient accumulation
# batch_size: 42            # batch size (feats_type=raw)
batch_size: 56
drop_last: true           # drop last batch
batch_type: unsorted      # how to make batch
grad_clip: -1             # gradient clipping norm
disc_grad_clip: -1        # gradient clipping norm
grad_noise: false         # whether to use gradient noise injection
sort_in_batch: descending # how to sort data in making batch
sort_batch: descending    # how to sort created batches
num_workers: 16           # number of workers of data loader
use_amp: false            # whether to use pytorch amp
log_interval: 50          # log interval in iterations
keep_nbest_models: 60   # number of models to keep
num_att_plot: 0           # number of attention figures to be saved in every check
seed: 0                   # random seed number
patience: null            # patience for early stopping
unused_parameters: true   # needed for multi gpu case
best_model_criterion:     # criterion to save the best models
-   - valid
    - generator_multi_spectral_recon_loss
    - min
cudnn_deterministic: false # setting to false accelerates the training speed but makes it non-deterministic
                           # in the case of GAN-TTS training, we strongly recommend setting to false
cudnn_benchmark: false     # setting to true might acdelerate the training speed but sometimes decrease it
                           # therefore, we set to false as a default (recommend trying both cases)
